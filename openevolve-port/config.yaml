# Configuration for HuggingFace prompt optimization
# Based on optimized settings from config2.yaml

# General settings
max_iterations: 50
checkpoint_interval: 10
log_level: "INFO"
diff_based_evolution: false  # Full rewrite mode (best for prompt optimization)
max_code_length: 10000
language: "text"  # Explicitly set language to text for prompt evolution

# LLM Configuration
llm:
  api_base: "https://api.openai.com/v1"  # Or your LLM provider
  models:
    - name: "gpt-4.1-mini"
      weight: 1.0
  temperature: 0.7
  max_tokens: 4000
  timeout: 120

# Prompt Configuration - Optimal settings discovered
prompt:
  template_dir: "templates"  # Use custom templates for prompt evolution
  num_top_programs: 3      # Best balance
  num_diverse_programs: 2  # Best balance
  include_artifacts: true  # +20.7% improvement when enabled

  # System message for prompt evolution
  system_message: |
    You are an expert prompt engineer. Your task is to revise an group of existing prompts designed for large language models (LLMs), without being explicitly told what the task is.

    Your improvements should:

    * Infer the intended task and expected output format based on the structure and language of the original prompts.
    * Clarify vague instructions, eliminate ambiguity, and improve overall interpretability for the LLM.
    * Strengthen alignment between the prompt and the desired task outcome, ensuring more consistent and accurate responses.
    * Improve robustness against edge cases or unclear input phrasing.
    * If helpful, include formatting instructions, boundary conditions, or illustrative examples that reinforce the LLM's expected behavior.
    * Avoid adding unnecessary verbosity or assumptions not grounded in the original prompt.
    * Use feedback to help reduce known errors

    The revised prompt should maintain the same input interface but be more effective, reliable, and production-ready for LLM use.

    Return only the improved prompts in the specified format. Do not include explanations or additional comments. Your output should be a clean, high-quality replacement that enhances clarity, consistency, and LLM performance.
    All prompts must start with >>> PROMPT_START {prompt_name}. The names of the prompts remain constant and should not change.

# Database Configuration
database:
  population_size: 1000
  archive_size: 100
  num_islands: 4

  feature_dimensions: ["prompt_length"]
  feature_bins: 10  # 10x10 grid = 100 cells

  # Selection parameters - Optimal ratios from testing
  elite_selection_ratio: 0.1   # 10% elite selection
  exploration_ratio: 0.3       # 30% exploration
  exploitation_ratio: 0.6      # 60% exploitation

  # Migration parameters - Optimal settings
  migration_interval: 10
  migration_rate: 0.1

# Evaluator Configuration
evaluator:
  timeout: 1800
  max_retries: 3
  parallel_evaluations: 4
  cascade_evaluation: false
